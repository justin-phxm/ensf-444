{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Justin Pham   \n",
        "* **UCID** = 30139323\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **27**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** missing data is generally unfavourable in machine learning since it can result in more inaccurate models and more difficult to tune and train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age          0\n",
            "sex          0\n",
            "cp           0\n",
            "trestbps     1\n",
            "chol        23\n",
            "fbs          8\n",
            "restecg      1\n",
            "thalach      1\n",
            "exang        1\n",
            "oldpeak      0\n",
            "num          0\n",
            "dtype: int64\n",
            "age          38\n",
            "sex           2\n",
            "cp            4\n",
            "trestbps     31\n",
            "chol        153\n",
            "fbs           2\n",
            "restecg       3\n",
            "thalach      71\n",
            "exang         2\n",
            "oldpeak      10\n",
            "num           2\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1.1\n",
        "# Add necessary code here.\n",
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "threshHold = 0.6 * len(df)\n",
        "df.dropna(thresh=threshHold, axis=1, inplace=True)\n",
        "\n",
        "print(df.isna().sum())\n",
        "# print number of unique values\n",
        "print(df.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** I chose simpleImputer because it is fast and easy to use. I am only filling in missing data with the mean of that column. Because we filled in missing data with the mean, this may we result in our model having slightly less accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "num         0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# 1.2\n",
        "# Add necessary code here.\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "print(df.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numerical_features:  Index(['age', 'cp', 'trestbps', 'chol', 'restecg', 'thalach', 'oldpeak'], dtype='object')\n",
            "categorial_features:  Index([], dtype='object')\n",
            "binary_features:  Index(['sex', 'fbs', 'exang'], dtype='object')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "age          38\n",
              "sex           2\n",
              "cp            4\n",
              "trestbps     31\n",
              "chol        153\n",
              "fbs           2\n",
              "restecg       3\n",
              "thalach      71\n",
              "exang         2\n",
              "oldpeak      10\n",
              "dtype: int64"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.3\n",
        "# Add necessary code here.\n",
        "y = df['num']\n",
        "X = df.drop(columns='num')\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
        "# get features from X where column is numerical\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorial_features = X.select_dtypes(include=['object']).columns\n",
        "# Binary features is the features in numerical features that have only 2 unique values\n",
        "binary_features = X[numerical_features].nunique()[X[numerical_features].nunique() == 2].index\n",
        "# Drop the binary features from numerical_features\n",
        "numerical_features = numerical_features.drop(binary_features)\n",
        "\n",
        "ss = ColumnTransformer(\n",
        "    transformers=[(\"ss\", StandardScaler(), numerical_features)],\n",
        ")\n",
        "ohe = ColumnTransformer(\n",
        "    transformers=[(\"ohe\", OneHotEncoder(), categorial_features)]\n",
        ")\n",
        "pt = ColumnTransformer(\n",
        "    transformers=[(\"pt\", \"passthrough\", binary_features)]\n",
        ")\n",
        "print(\"numerical_features: \", numerical_features)\n",
        "print(\"categorial_features: \", categorial_features)\n",
        "print(\"binary_features: \", binary_features)\n",
        "X.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** \n",
        "\n",
        "The three modles I have chosen are decision tree classifier, support vector classifier, and KNN classifier.\n",
        "\n",
        "\n",
        "I chose decision tree classifier because decision trees are great options for classification and regression models. This makes them flexible for different types of data.\n",
        "\n",
        "Strengths:\n",
        "1. Works for both classficiation and regression, making them very flexible\n",
        "1. Can be combined in forests for even greater accuracy and performance.\n",
        "1. Easy to interpret and visualize, making it clear how decisions are made.\n",
        "\n",
        "Weaknesses:\n",
        "1. Prone to overfitting if not properly tuned.\n",
        "1. Not very powerful on it's own, usually combined with forests.\n",
        "1. Sensitive to changes in the training data, potentially leading to unstable predictions for small variations.\n",
        "\n",
        "I chose support vector classifier because SVC can capture relationships between different features.\n",
        "\n",
        "Strengths:\n",
        "1. Powerful for high-dimensional data and works well with small datasets.\n",
        "1. Can handle complex non-linear relationships between features using kernel functions.\n",
        "\n",
        "Weaknesses:\n",
        "1. Hard to explain and understand\n",
        "1. Can be computationally expensive for very large datasets due to the optimization process.\n",
        "1. Choosing the right kernel function and its hyperparameters can be crucial for performance and can involve trial-and-error.\n",
        "\n",
        "\n",
        "I chose knn classifier because it is simple to explain and understand.\n",
        "\n",
        "Strengths:\n",
        "1. Easy to explain and understand\n",
        "1. Makes no assumptions about the underlying data distribution.\n",
        "1. Minimal tuning necessary\n",
        "\n",
        "\n",
        "Weaknesses:\n",
        "1. Performance can be slow for high-dimensional data due to distance calculations for all neighbors.\n",
        "1. Can suffer from the \"curse of dimensionality\" in high dimensions, where distances become meaningless.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Add necessary code here.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import tree, svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "pipe_dt = Pipeline( [('ss', ss), ('ohe', ohe), (\"pt\", pt), (\"dt\", tree.DecisionTreeClassifier())] )\n",
        "pipe_svm = Pipeline([('ss', ss), ('ohe', ohe), (\"pt\", pt), (\"svm\", svm.SVC())])\n",
        "pipe_knn = Pipeline([('ss', ss), ('ohe', ohe), (\"pt\", pt), (\"knn\", KNeighborsClassifier())])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.2** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\nAll the 20 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 482, in _get_column_indices\n    all_columns = X.columns\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 471, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 408, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/home/codespace/.local/lib/python3.10/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1303, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 295, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 906, in fit_transform\n    self._validate_column_callables(X)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 496, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 484, in _get_column_indices\n    raise ValueError(\nValueError: Specifying the columns using strings is only supported for dataframes.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 22\u001b[0m\n\u001b[1;32m     14\u001b[0m param_grid_svm \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m : [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m grid_search_dt \u001b[38;5;241m=\u001b[39m GridSearchCV(pipe_dt, param_grid_dt, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mgrid_search_dt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(type(X_train))\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(type(y_train))\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    945\u001b[0m     )\n\u001b[0;32m--> 947\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    535\u001b[0m     )\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 20 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n20 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 482, in _get_column_indices\n    all_columns = X.columns\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 471, in fit\n    Xt = self._fit(X, y, routed_params)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 408, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/home/codespace/.local/lib/python3.10/site-packages/joblib/memory.py\", line 353, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1303, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 295, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 906, in fit_transform\n    self._validate_column_callables(X)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 496, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"/home/codespace/.local/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 484, in _get_column_indices\n    raise ValueError(\nValueError: Specifying the columns using strings is only supported for dataframes.\n"
          ]
        }
      ],
      "source": [
        "# 2.2\n",
        "# Add necessary code here.\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=RANDOM_STATE)\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"dt__max_depth\": [None, 3, 7, 11],\n",
        "}\n",
        "param_grid_knn = {\n",
        "    \"n_neighbours\": [3, 5, 7, 9, 11],\n",
        "}\n",
        "param_grid_svm = {\n",
        "    \"C\": [1, 10, 100],\n",
        "    \"gamma\": [1, 10, 100],\n",
        "    \"kernel\" : [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
        "}\n",
        "\n",
        "grid_search_dt = GridSearchCV(pipe_dt, param_grid_dt, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
        "\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "# print(type(X_train))\n",
        "# print(type(y_train))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.3** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [],
      "source": [
        "# 2.3\n",
        "# Add necessary code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.4** ....................."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
